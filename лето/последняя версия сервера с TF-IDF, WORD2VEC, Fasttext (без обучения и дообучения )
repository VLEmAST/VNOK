import os
import json
import re
import time
import threading
import heapq
import concurrent.futures
import numpy as np
from flask import Flask, request, jsonify
from typing import List, Dict, Any
import gensim.models.word2vec
import gensim.models.fasttext
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
import pymorphy3

# ==============================
# –ö–û–ù–°–¢–ê–ù–¢–´ –ú–û–î–ï–õ–ï–ô –ò –ê–õ–ì–û–†–ò–¢–ú–û–í
# ==============================

# –ù–∞–∑–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
MODEL_WORD2VEC = "wordtwovec"
MODEL_FASTTEXT = "fasttext"
MODEL_TFIDF = "tfidf"

# –î–æ—Å—Ç—É–ø–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
AVAILABLE_MODELS = [MODEL_WORD2VEC, MODEL_FASTTEXT, MODEL_TFIDF]

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
DEFAULT_MODEL = MODEL_TFIDF
DEFAULT_TOP_N = 10

# –ù–∞–∑–≤–∞–Ω–∏—è –ø–æ–ª–µ–π –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö (–¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è)
FIELD_THREAD = 'ThreadId'
FIELD_VULNERABILITY = 'VulnerabilitieId'
FIELD_TACTIC = 'TacticId'
FIELD_PROTECTION_MEASURE = 'ProtectionMeasureId'
FIELD_TECHNOLOGY = 'TechnologyId'

# –¢–∏–ø—ã DTO –∏ –∏—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ñ—É–Ω–∫—Ü–∏—è–º –æ–±—Ä–∞–±–æ—Ç–∫–∏
DTO_TYPE_TO_CONFIG = {
    14: {'field': FIELD_TECHNOLOGY, 'function': 'get_top_n_outcomes'},
    12: {'field': FIELD_PROTECTION_MEASURE, 'function': 'get_top_n_for_category'},
    8: {'field': FIELD_TACTIC, 'function': 'get_top_n_tactics'},
    5: {'field': FIELD_THREAD, 'function': 'get_top_n_for_category'},
    6: {'field': FIELD_VULNERABILITY, 'function': 'get_top_n_for_category'}
}

# –ù–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö
DB_FILES = {
    "Thread": "threatDb.json",
    "Vulnerability": "vulnerabilitieDb.json",
    "Tactic": "tacticDb.json",
    "ProtectionMeasure": "protectionMeasureDb.json",
    "Technology": "outcomesDb.json"
}

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
TOKENIZE_MIN_LENGTH = 2
TOKENIZE_NGRAM_RANGE = (1, 3)
TOKENIZE_MAX_FEATURES = 10000

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–µ–π (—Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫–∞–∫ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
WORD2VEC_VECTOR_SIZE = 300
WORD2VEC_WINDOW = 5
WORD2VEC_MIN_COUNT = 2
WORD2VEC_EPOCHS = 10

FASTTEXT_VECTOR_SIZE = 300
FASTTEXT_WINDOW = 5
FASTTEXT_MIN_COUNT = 2
FASTTEXT_EPOCHS = 10

# ==============================
# –ö–û–ù–ï–¶ –ö–û–ù–°–¢–ê–ù–¢
# ==============================

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ NLTK –¥–∞–Ω–Ω—ã—Ö
try:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
    nltk_available = True
except Exception as e:
    print(f"‚ö†Ô∏è NLTK –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é.")
    nltk_available = False

    # –ü—Ä–æ—Å—Ç–∞—è –∑–∞–º–µ–Ω–∞ —Ñ—É–Ω–∫—Ü–∏–π NLTK
    def word_tokenize(text):
        return re.findall(r'\b\w+\b', text.lower())

    class StopwordsFallback:
        def words(self, language):
            return set([
                '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞',
                '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ',
                '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É',
                '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å',
                '—É–∂', '–≤–∞–º', '—Å–∫–∞–∑–∞–ª', '–≤–µ–¥—å', '—Ç–∞–º', '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π', '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç',
                '–≥–¥–µ', '–µ—Å—Ç—å', '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è', '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º', '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±', '–±–µ–∑',
                '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç', '–∂', '—Ç–æ–≥–¥–∞', '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ',
                '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º', '–Ω–∏–º', '–∑–¥–µ—Å—å', '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º',
                '—á—Ç–æ–±—ã', '–Ω–µ–µ', '—Å–µ–π—á–∞—Å', '–±—ã–ª–∏', '–∫—É–¥–∞', '–∑–∞—á–µ–º', '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü',
                '–¥–≤–∞', '–æ–±', '–¥—Ä—É–≥–æ–π', '—Ö–æ—Ç—å', '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å', '–ø—Ä–æ',
                '–≤—Å–µ–≥–æ', '–Ω–∏—Ö', '–∫–∞–∫–∞—è', '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ', '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é',
                '—ç—Ç–æ–π', '–ø–µ—Ä–µ–¥', '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è', '—Ç–∞–∫–æ–π', '–∏–º', '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞',
                '–∫–æ–Ω–µ—á–Ω–æ', '–≤—Å—é', '–º–µ–∂–¥—É'
            ])

    stopwords = StopwordsFallback()

app = Flask(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Flask –Ω–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON
app.json.ensure_ascii = False
app.json.sort_keys = False
app.json.compact = False
app.json.indent = 2

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—É—Ç–µ–π
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
DATABASE_DIR = os.path.join(SCRIPT_DIR, 'Database')
MODELS_DIR = os.path.join(SCRIPT_DIR, 'Models')

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É Models –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
os.makedirs(MODELS_DIR, exist_ok=True)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –º–æ–¥–µ–ª–∏
description_vector_cache = {}
cache_lock = threading.Lock()
model = None
model_type = None
model_vector_size = None  # –ù–æ–≤–æ–µ –ø–æ–ª–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
use_pos_tags = False

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
try:
    morph = pymorphy3.MorphAnalyzer()
    morph_available = True
except Exception as e:
    print(f"‚ö†Ô∏è Pymorphy3 –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É.")
    morph = None
    morph_available = False

# –°—Ç–æ–ø-—Å–ª–æ–≤–∞
try:
    stop_words = set(stopwords.words('russian')) | {'–±—ã—Ç—å', '—ç—Ç–æ', '—Ç–∞–∫–æ–π', '–∫–æ—Ç–æ—Ä—ã–π', '–≤—Å–µ', '—ç—Ç–æ—Ç'}
except:
    stop_words = StopwordsFallback().words('russian')

# TF-IDF —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
db_vectors = {}
db_documents = {}
db_guids = {}
db_sources = {}

# –ë–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Word2Vec/FastText
dbs = {}

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Word2Vec –∏–ª–∏ FastText
def load_model(model_path, model_name):
    global model, model_type, model_vector_size, use_pos_tags
    
    if not os.path.exists(model_path):
        print(f"‚ùå –§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
        return None
    
    if model is not None and model_type == model_name and hasattr(model, 'filename') and model.filename == model_path:
        return model

    try:
        if model_name == MODEL_WORD2VEC:
            print(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Word2Vec –º–æ–¥–µ–ª–∏ –∏–∑ {model_path}")
            try:
                model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)
                model_vector_size = model.vector_size
                print(f"‚úÖ –ú–æ–¥–µ–ª—å Word2Vec —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (–±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç), —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {model_vector_size}")
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞–∫ –±–∏–Ω–∞—Ä–Ω—ã–π Word2Vec: {e}")
                try:
                    model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=False)
                    model_vector_size = model.vector_size
                    print(f"‚úÖ –ú–æ–¥–µ–ª—å Word2Vec —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (—Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç), —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {model_vector_size}")
                except Exception as e:
                    print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–π Word2Vec: {e}")
                    try:
                        model = gensim.models.Word2Vec.load(model_path)
                        model_vector_size = model.wv.vector_size
                        print(f"‚úÖ –ú–æ–¥–µ–ª—å Word2Vec —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (Gensim —Ñ–æ—Ä–º–∞—Ç), —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {model_vector_size}")
                    except Exception as e:
                        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞–∫ Gensim Word2Vec: {e}")
                        return None
            
            if model:
                use_pos_tags = any('_' in key for key in list(model.key_to_index.keys())[:100])
                print(f"‚ÑπÔ∏è –ú–æ–¥–µ–ª—å Word2Vec –∏—Å–ø–æ–ª—å–∑—É–µ—Ç POS-—Ç—ç–≥–∏: {use_pos_tags}")
                
        elif model_name == MODEL_FASTTEXT:
            print(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ FastText –º–æ–¥–µ–ª–∏ –∏–∑ {model_path}")
            try:
                model = gensim.models.fasttext.load_facebook_vectors(model_path)
                model_vector_size = model.vector_size
                print(f"‚úÖ –ú–æ–¥–µ–ª—å FastText —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —á–µ—Ä–µ–∑ gensim, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {model_vector_size}")
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å FastText —á–µ—Ä–µ–∑ gensim: {e}")
                try:
                    model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)
                    model_vector_size = model.vector_size
                    print(f"‚úÖ –ú–æ–¥–µ–ª—å FastText –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–∞–∫ Word2Vec —Ñ–æ—Ä–º–∞—Ç, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {model_vector_size}")
                except Exception as e:
                    print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å FastText: {e}")
                    return None
        else:
            print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –∏–º—è –º–æ–¥–µ–ª–∏ '{model_name}'")
            return None

        model.filename = model_path
        model_type = model_name
        return model
        
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        return None

# –ó–∞–≥—Ä—É–∑–∫–∞ JSON —Ñ–∞–π–ª–æ–≤
def load_json(file_name, dto_type):
    file_path = os.path.join(DATABASE_DIR, file_name)
    try:
        if not os.path.exists(file_path):
            print(f"‚ùå –§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return {"DtoType": dto_type, "Value": []}
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            print(f"‚úÖ –§–∞–π–ª {file_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
            return data
    except json.JSONDecodeError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª {file_path} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –≤–∞–ª–∏–¥–Ω—ã–º JSON - {e}")
        return {"DtoType": dto_type, "Value": []}
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}")
        return {"DtoType": dto_type, "Value": []}

# –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
def simple_tokenize(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', ' ', text)
    words = re.findall(r'\b\w+\b', text)
    return [word for word in words if word not in stop_words]

# –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è Word2Vec/FastText
def get_sentence_vector(text, model, model_name):
    if model is None:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        vector_size = model_vector_size if model_vector_size is not None else WORD2VEC_VECTOR_SIZE
        return np.zeros(vector_size)
    
    cache_key = f"{text}_{model_name}"
    
    with cache_lock:
        if cache_key in description_vector_cache:
            return description_vector_cache[cache_key]
    
    vectors = []
    
    if model_name == MODEL_WORD2VEC:
        words = simple_tokenize(text)
        for word in words:
            if morph_available and morph:
                try:
                    parsed = morph.parse(word)[0]
                    normal_form = parsed.normal_form
                    for word_form in [word, normal_form]:
                        if word_form in model:
                            vectors.append(model[word_form])
                            break
                    if use_pos_tags and morph_available:
                        pos = parsed.tag.POS
                        if pos:
                            pos_map = {
                                'NOUN': 'NOUN', 'VERB': 'VERB', 'ADJF': 'ADJ', 'ADJS': 'ADJ',
                                'ADVB': 'ADV', 'INFN': 'VERB', 'PRTF': 'ADJ', 'PRTS': 'ADJ'
                            }
                            pos_suffix = pos_map.get(pos, '')
                            if pos_suffix:
                                word_with_pos = f"{normal_form}_{pos_suffix}"
                                if word_with_pos in model:
                                    vectors.append(model[word_with_pos])
                except Exception as e:
                    if word in model:
                        vectors.append(model[word])
            else:
                if word in model:
                    vectors.append(model[word])
                    
    else:  # FastText
        words = text.split()
        for word in words:
            word_clean = re.sub(r'[^\w]', '', word.lower())
            if word_clean and word_clean in model:
                vectors.append(model[word_clean])
    
    if not vectors:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        vector_size = model_vector_size if model_vector_size is not None else WORD2VEC_VECTOR_SIZE
        vector = np.zeros(vector_size)
    else:
        vector = np.mean(vectors, axis=0)
    
    with cache_lock:
        description_vector_cache[cache_key] = vector
    
    return vector

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è Word2Vec/FastText
def get_top_n_for_category(db_data, input_vector, input_norm, model, model_name, n=DEFAULT_TOP_N):
    value = db_data.get('Value', [])
    if not value:
        return []

    top_results = []
    
    for item in value:
        desc = item.get('Description', '')
        if not desc:
            continue
            
        desc_vector = get_sentence_vector(desc, model, model_name)
        
        if np.any(desc_vector):
            desc_norm = np.linalg.norm(desc_vector)
            if desc_norm > 0:
                dot_product = np.dot(input_vector, desc_vector)
                similarity = dot_product / (input_norm * desc_norm)
                
                if len(top_results) < n:
                    heapq.heappush(top_results, (similarity, item.get('GuidId')))
                else:
                    if similarity > top_results[0][0]:
                        heapq.heapreplace(top_results, (similarity, item.get('GuidId')))

    top_results.sort(key=lambda x: x[0], reverse=True)
    return top_results

def get_top_n_outcomes(db_data, input_vector, input_norm, model, model_name, n=DEFAULT_TOP_N):
    value = db_data.get('Value', {})
    techns = value.get('Technologys', [])
    if not techns:
        return []

    top_results = []
    
    for item in techns:
        desc = item.get('Description', '')
        if not desc:
            continue
            
        desc_vector = get_sentence_vector(desc, model, model_name)
        
        if np.any(desc_vector):
            desc_norm = np.linalg.norm(desc_vector)
            if desc_norm > 0:
                dot_product = np.dot(input_vector, desc_vector)
                similarity = dot_product / (input_norm * desc_norm)
                
                if len(top_results) < n:
                    heapq.heappush(top_results, (similarity, item.get('GuidId')))
                else:
                    if similarity > top_results[0][0]:
                        heapq.heapreplace(top_results, (similarity, item.get('GuidId')))

    top_results.sort(key=lambda x: x[0], reverse=True)
    return top_results

def get_top_n_tactics(db_data, input_vector, input_norm, model, model_name, n=10):
    value = db_data.get('Value', [])
    if not value:
        return []

    top_results = []
    
    for item in value:
        similarities = []  # –°–ø–∏—Å–æ–∫ —Å—Ö–æ–¥—Å—Ç–≤ –¥–ª—è —Ç–∞–∫—Ç–∏–∫–∏ –∏ –µ—ë —Ç–µ—Ö–Ω–∏–∫
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–ø–∏—Å–∞–Ω–∏—è —Ç–∞–∫—Ç–∏–∫–∏
        desc = item.get('Description', '')
        if desc:
            desc_vector = get_sentence_vector(desc, model, model_name)
            if np.any(desc_vector):
                desc_norm = np.linalg.norm(desc_vector)
                if desc_norm > 0:
                    dot_product = np.dot(input_vector, desc_vector)
                    similarity = dot_product / (input_norm * desc_norm)
                    similarities.append(similarity)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–ø–∏—Å–∞–Ω–∏–π —Ç–µ—Ö–Ω–∏–∫
        for tech in item.get('Techniques', []):
            desc = tech.get('Description', '')
            if desc:
                desc_vector = get_sentence_vector(desc, model, model_name)
                if np.any(desc_vector):
                    desc_norm = np.linalg.norm(desc_vector)
                    if desc_norm > 0:
                        dot_product = np.dot(input_vector, desc_vector)
                        similarity = dot_product / (input_norm * desc_norm)
                        similarities.append(similarity)
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å—Ö–æ–¥—Å—Ç–≤–∞, –≤—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç (—Å—Ä–µ–¥–Ω–µ–µ)
        if similarities:
            overall_similarity = sum(similarities) / len(similarities)
            top_results.append((overall_similarity, item.get('GuidId')))
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–π, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–ª–∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º 0
            continue

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –æ–±—â–µ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –∏ –±–µ—Ä—ë–º —Ç–æ–ø-N
    top_results.sort(key=lambda x: x[0], reverse=True)
    return top_results[:n]

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è TF-IDF
def load_entities_from_json(filename: str, db_key: str):
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ JSON —Ñ–∞–π–ª–∞ –¥–ª—è TF-IDF"""
    entities = []
    file_path = os.path.join(DATABASE_DIR, filename)

    if not os.path.exists(file_path):
        print(f"‚ùå –§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return [], [], []

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {filename}: {e}")
        return [], [], []

    def traverse(obj):
        if isinstance(obj, dict):
            guid = obj.get("GuidId", "").strip()
            desc = obj.get("Description", "").strip()
            if guid and desc:
                entities.append((guid, desc))
            for v in obj.values():
                traverse(v)
        elif isinstance(obj, list):
            for item in obj:
                traverse(item)

    traverse(data)
    guids = [item[0] for item in entities]
    texts = [item[1] for item in entities]
    sources = [filename] * len(guids)
    return texts, guids, sources

def initialize_databases():
    """–ï–¥–∏–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—Å–µ—Ö –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö"""
    global dbs
    
    print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Word2Vec/FastText
    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    outcomes_db = load_json('outcomesDb.json', 14)
    protection_measure_db = load_json('protectionMeasureDb.json', 12)
    tactic_db = load_json('tacticDb.json', 8)
    threat_db = load_json('threatDb.json', 5)
    vulnerabilitie_db = load_json('vulnerabilitieDb.json', 6)

    dbs = {
        14: outcomes_db,
        12: protection_measure_db,
        8: tactic_db,
        5: threat_db,
        6: vulnerabilitie_db
    }
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TF-IDF –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö
    print("üî§ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TF-IDF –≤–µ–∫—Ç–æ—Äizers...")
    for db_key, filename in DB_FILES.items():
        print(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ {db_key} –∏–∑ {filename}...")
        docs, guids, sources = load_entities_from_json(filename, db_key)
        if not docs:
            print(f"‚ö†Ô∏è –í {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è.")
            continue

        vectorizer = TfidfVectorizer(
            lowercase=True,
            stop_words=None,
            token_pattern=r'[–∞-—è–ê-–Ø—ë–Åa-zA-Z]{2,}',
            ngram_range=TOKENIZE_NGRAM_RANGE,
            max_features=TOKENIZE_MAX_FEATURES
        )
        vectorizer.fit(docs)

        db_documents[db_key] = docs
        db_guids[db_key] = guids
        db_sources[db_key] = sources
        db_vectors[db_key] = vectorizer

        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(docs)} –æ–ø–∏—Å–∞–Ω–∏–π –¥–ª—è {db_key}")
    
    print("‚úÖ –í—Å–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")

def find_top_n_matches_tfidf(query: str, db_key: str, n: int = DEFAULT_TOP_N) -> List[Dict[str, Any]]:
    if db_key not in db_vectors:
        return []

    vectorizer = db_vectors[db_key]
    documents = db_documents[db_key]
    guids = db_guids[db_key]

    query_tfidf = vectorizer.transform([query])
    tfidf_matrix = vectorizer.transform(documents)
    similarities = cosine_similarity(query_tfidf, tfidf_matrix).flatten()

    matches = [
        {"similarity": float(sim), "GuidId": guid}
        for sim, guid in zip(similarities, guids)
    ]

    matches.sort(key=lambda x: x["similarity"], reverse=True)
    return matches[:n]

# –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
FUNCTION_MAP = {
    'get_top_n_for_category': get_top_n_for_category,
    'get_top_n_outcomes': get_top_n_outcomes,
    'get_top_n_tactics': get_top_n_tactics
}

# –ï–î–ò–ù–û–†–ê–ó–û–í–ê–Ø –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ë–ê–ó –î–ê–ù–ù–´–•
initialize_databases()

@app.route('/Matcher', methods=['POST'])
@app.route('/Matcher', methods=['POST'])
def matcher():
    global model, model_type, model_vector_size
    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': '–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –¥–∞–Ω–Ω—ã–µ JSON'}), 400

        text_description = data.get('TextDescription') or data.get('query')
        model_name = data.get('ModelName')
        used_model_path = data.get('UsedModel')
        filtering_cvss = data.get('FilteringCvss', False)

        if text_description is None or model_name is None:
            return jsonify({'error': '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (TextDescription/query –∏ ModelName –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã)'}), 400

        text_description = str(text_description).strip()
        model_name = str(model_name).lower()
        filtering_cvss = bool(filtering_cvss)

        if not text_description or text_description.isspace():
            print(f"‚ùå –û—à–∏–±–∫–∞: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã: '{text_description}'")
            return jsonify({'error': '–í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã'}), 400

        if model_name not in AVAILABLE_MODELS:
            return jsonify({'error': f"–ù–µ–¥–æ–ø—É—Å—Ç–∏–º–æ–µ –∏–º—è –º–æ–¥–µ–ª–∏: {model_name}. –û–∂–∏–¥–∞–µ—Ç—Å—è {', '.join(AVAILABLE_MODELS)}"}), 400

        start_time = time.time()

        # –î–ª—è TF-IDF –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º UsedModel, –µ—Å–ª–∏ –æ–Ω –ø—É—Å—Ç–æ–π
        if model_name == MODEL_TFIDF:
            if used_model_path:
                print(f"‚ÑπÔ∏è –î–ª—è TF-IDF –º–æ–¥–µ–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä UsedModel –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è: {used_model_path}")
            
            top_results = {}
            for db_key in DB_FILES.keys():
                if db_key == "Tactic":
                    # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Ç–∞–∫—Ç–∏–∫
                    tactic_db = dbs[8]
                    vectorizer = db_vectors.get(db_key)
                    if vectorizer:
                        matches = get_top_n_tactics_tfidf(text_description, tactic_db, vectorizer, DEFAULT_TOP_N)
                    else:
                        matches = [(0.0, None)] * DEFAULT_TOP_N
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç {"similarity": sim, "GuidId": guid}
                    top_results[db_key] = [{"similarity": sim, "GuidId": guid} for sim, guid in matches]
                else:
                    matches = find_top_n_matches_tfidf(text_description, db_key, DEFAULT_TOP_N)
                    if not matches:
                        matches = [{"similarity": 0.0, "GuidId": None}] * DEFAULT_TOP_N
                    top_results[db_key] = matches

        elif model_name in [MODEL_WORD2VEC, MODEL_FASTTEXT]:
            if not used_model_path:
                return jsonify({'error': '–î–ª—è word2vec/fasttext –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å UsedModel - –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª–∏'}), 400
            
            if model is None or model_type != model_name or (hasattr(model, 'filename') and model.filename != used_model_path):
                model = load_model(used_model_path, model_name)
                if model is None:
                    return jsonify({'error': f'–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏: {used_model_path}'}), 500

            input_vector = get_sentence_vector(text_description, model, model_name)
            if not np.any(input_vector):
                print(f"‚ö†Ô∏è –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç '{text_description}' –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω –º–æ–¥–µ–ª—å—é {model_name}")
                return jsonify({'error': f'–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ –º–æ–¥–µ–ª—å—é {model_name}'}), 500

            input_norm = np.linalg.norm(input_vector)
            if input_norm == 0:
                print(f"‚ö†Ô∏è –ù—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä –¥–ª—è —Ç–µ–∫—Å—Ç–∞ '{text_description}'")
                return jsonify({'error': f'–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ –º–æ–¥–µ–ª—å—é {model_name}'}), 500

            category_results = {}
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=min(len(DTO_TYPE_TO_CONFIG), 4)) as executor:
                future_to_dto = {}
                for dto_type, config in DTO_TYPE_TO_CONFIG.items():
                    if dto_type in dbs:
                        function_name = config['function']
                        function = FUNCTION_MAP.get(function_name)
                        if function:
                            future = executor.submit(
                                function, 
                                dbs[dto_type], 
                                input_vector, 
                                input_norm,
                                model,
                                model_name
                            )
                            future_to_dto[future] = (dto_type, config['field'])
                
                for future in concurrent.futures.as_completed(future_to_dto):
                    dto_type, field = future_to_dto[future]
                    try:
                        top_results = future.result()
                        category_results[field] = top_results
                        print(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ {field}, –Ω–∞–π–¥–µ–Ω–æ {len(top_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {field}: {e}")
                        category_results[field] = [(0.0, None)] * DEFAULT_TOP_N
        else:
            return jsonify({'error': f'–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –º–æ–¥–µ–ª—å: {model_name}'}), 400

        response = []
        
        for i in range(DEFAULT_TOP_N):
            similarities = []
            response_item = {}
            
            field_mapping = {
                'Thread': FIELD_THREAD,
                'Vulnerability': FIELD_VULNERABILITY,
                'Tactic': FIELD_TACTIC,
                'ProtectionMeasure': FIELD_PROTECTION_MEASURE,
                'Technology': FIELD_TECHNOLOGY
            }
            
            print(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–∑–∏—Ü–∏–∏ {i}:")
            
            for db_key, field_name in field_mapping.items():
                if model_name == MODEL_TFIDF:
                    field_results = top_results.get(db_key, [])
                else:
                    field_results = category_results.get(field_name, [])

                if i < len(field_results):
                    similarity = field_results[i]["similarity"] if model_name == MODEL_TFIDF else field_results[i][0]
                    guid_id = field_results[i]["GuidId"] if model_name == MODEL_TFIDF else field_results[i][1]
                    response_item[field_name] = guid_id
                    
                    # –û–¢–õ–ê–î–ö–ê: –≤—ã–≤–æ–¥–∏–º –≤—Å–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã, –¥–∞–∂–µ –Ω—É–ª–µ–≤—ã–µ
                    print(f"   {field_name}: similarity = {similarity}, guid = {guid_id}")
                    
                    if similarity != 0:
                        similarities.append(abs(similarity))
                    else:
                        print(f"   ‚ö†Ô∏è {field_name}: –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç {similarity} –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω (<= 0)")
                else:
                    response_item[field_name] = None
                    print(f"   {field_name}: –Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            
            # –û–¢–õ–ê–î–ö–ê: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–ø–∏—Å–∫–µ similarities
            print(f"üìä similarities —Å–ø–∏—Å–æ–∫: {similarities}")
            print(f"üìä –î–ª–∏–Ω–∞ similarities: {len(similarities)}")
            
            coefficient = sum(similarities) / len(similarities) if similarities  else 0.0
            response_item['Coefficient'] = round(coefficient, 6)
            
            print(f"üéØ –ò—Ç–æ–≥–æ–≤—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è –ø–æ–∑–∏—Ü–∏–∏ {i}: {coefficient}")
            response.append(response_item)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω—É–ª–µ–≤–æ–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        all_coefficients_zero = all(item['Coefficient'] == 0.0 for item in response)
        if all_coefficients_zero:
            print(f"‚ùå –í—Å–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã —Ä–∞–≤–Ω—ã 0 –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è: '{text_description}'")
            return jsonify({'error': f'–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ –º–æ–¥–µ–ª—å—é'}), 500

        processing_time = time.time() - start_time
        print(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time:.3f} —Å–µ–∫—É–Ω–¥")
        print(f"üìä –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ {len(response)} –æ–±—ä–µ–∫—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞")
      
        return jsonify(response), 200

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞: {e}")
        return jsonify({'error': f'–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞: {str(e)}'}), 500

import fasttext

@app.route('/Train', methods=['POST'])
def train_model():
    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': '–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –¥–∞–Ω–Ω—ã–µ JSON'}), 400
        
        model_name = data.get('NameAlgorithm')
        if not model_name:
            return jsonify({'error': '–ù–µ —É–∫–∞–∑–∞–Ω–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (ModelName)'}), 400
        
        model_name = model_name.lower()
        if model_name not in [MODEL_WORD2VEC, MODEL_FASTTEXT]:
            return jsonify({'error': f'–ù–µ–¥–æ–ø—É—Å—Ç–∏–º–æ–µ –∏–º—è –º–æ–¥–µ–ª–∏. –û–∂–∏–¥–∞–µ—Ç—Å—è "{MODEL_WORD2VEC}" –∏–ª–∏ "{MODEL_FASTTEXT}"'}), 400
        
        training_texts = []
        
        def collect_texts(data):
            if isinstance(data, dict):
                for key, value in data.items():
                    if isinstance(value, str) and key.lower() in ['description', 'name', 'title', 'methodname', 'usage', 'horizont', 'necessity', 'experience', '—Åharacteristic', 'effort']:
                        if value.strip() and len(value.strip()) > TOKENIZE_MIN_LENGTH:
                            training_texts.append(value.strip())
                    elif isinstance(value, (dict, list)):
                        collect_texts(value)
            elif isinstance(data, list):
                for item in data:
                    collect_texts(item)
        
        for filename in DB_FILES.values():
            file_path = os.path.join(DATABASE_DIR, filename)
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        db_data = json.load(f)
                    collect_texts(db_data)
                    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω —Ñ–∞–π–ª: {filename}, —Å–æ–±—Ä–∞–Ω–æ {len(training_texts)} —Ç–µ–∫—Å—Ç–æ–≤")
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {filename}: {e}")
        
        if not training_texts:
            return jsonify({'error': '–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Ç–µ–∫—Å—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö'}), 500
        
        temp_training_file = os.path.join(SCRIPT_DIR, 'fasttext_training_data.txt')
        
        try:
            with open(temp_training_file, 'w', encoding='utf-8') as f:
                for text in training_texts:
                    cleaned_text = re.sub(r'\s+', ' ', text.strip())
                    f.write(cleaned_text + '\n')
            
            print(f"üìÅ –°–æ–∑–¥–∞–Ω –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {temp_training_file}")
            print(f"üìä –í—Å–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(training_texts)}")
            
            if model_name == MODEL_WORD2VEC:
                tokenized_sentences = []
                for text in training_texts:
                    tokens = simple_tokenize(text)
                    if tokens:
                        tokenized_sentences.append(tokens)
                
                model = gensim.models.Word2Vec(
                    sentences=tokenized_sentences,
                    vector_size=WORD2VEC_VECTOR_SIZE,
                    window=WORD2VEC_WINDOW,
                    min_count=WORD2VEC_MIN_COUNT,
                    workers=4,
                    sg=1,
                    epochs=WORD2VEC_EPOCHS
                )
                model_path = os.path.join(MODELS_DIR, 'trained_word2vec_model.bin')
                model.wv.save_word2vec_format(model_path, binary=True)
                print(f"üíæ Word2Vec –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {model_path}")
                
            elif model_name == MODEL_FASTTEXT:
                model_path = os.path.join(MODELS_DIR, 'trained_fasttext_model.bin')
                
                model = fasttext.train_unsupervised(
                    input=temp_training_file,
                    model='skipgram',
                    dim=FASTTEXT_VECTOR_SIZE,
                    ws=FASTTEXT_WINDOW,
                    minCount=FASTTEXT_MIN_COUNT,
                    epoch=FASTTEXT_EPOCHS,
                    thread=4
                )
                
                model.save_model(model_path)
                print(f"üíæ FastText –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Facebook: {model_path}")
            
            response_data = {
                'ModelPath': model_path
            }
            
            return jsonify(response_data), 200
            
        finally:
            if os.path.exists(temp_training_file):
                os.remove(temp_training_file)
                print(f"üóëÔ∏è –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —É–¥–∞–ª–µ–Ω: {temp_training_file}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
        return jsonify({'error': f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {str(e)}'}), 500

@app.route('/health', methods=['GET'])
def health():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–µ—Ä–∞"""
    return jsonify({
        "status": "healthy",
        "nltk_available": nltk_available,
        "morph_available": morph_available,
        "model_loaded": model is not None,
        "model_type": model_type,
        "model_vector_size": model_vector_size,  # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        "tfidf_initialized": bool(db_vectors),
        "models_directory": MODELS_DIR,
        "database_directory": DATABASE_DIR,
        "available_models": AVAILABLE_MODELS,
        "default_model": DEFAULT_MODEL
    })

if __name__ == '__main__':
    print("üöÄ –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω")
    print(f"üìä NLTK –¥–æ—Å—Ç—É–ø–µ–Ω: {nltk_available}")
    print(f"üìä Pymorphy3 –¥–æ—Å—Ç—É–ø–µ–Ω: {morph_available}")
    print(f"üìÅ –ü–∞–ø–∫–∞ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö: {DATABASE_DIR}")
    print(f"üìÅ –ü–∞–ø–∫–∞ –º–æ–¥–µ–ª–µ–π: {MODELS_DIR}")
    print(f"üîß –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {', '.join(AVAILABLE_MODELS)}")
    print(f"‚öôÔ∏è –ú–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {DEFAULT_MODEL}")
    app.run(host='127.0.0.1', port=5000, debug=False, threaded=True)
